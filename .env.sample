# Code Q&A Configuration Sample
# Copy this file to .env and update with your settings

# ============================================================================
# AI Model Settings
# ============================================================================

# For Gemini Developer API:
# Your Gemini API key from Google AI Studio
GEMINI_API_KEY=your_api_key_here
# Alternative: GOOGLE_API_KEY=your_api_key_here

# For Vertex AI:
# Set these variables to use Vertex AI instead of the Gemini Developer API
# GOOGLE_GENAI_USE_VERTEXAI=true
# GOOGLE_CLOUD_PROJECT=your-project-id
# GOOGLE_CLOUD_LOCATION=us-central1

# AI model to use for responses (options: gemini-2.0-flash-001, gemini-1.5-pro, etc.)
CODE_QNA_MODEL=gemini-2.0-flash-001

# API version (optional: v1, v1alpha, etc.)
# Leave unset to use the default version
# CODE_QNA_API_VERSION=v1

# Model temperature (0.0-2.0): Lower = more focused, Higher = more creative
CODE_QNA_TEMPERATURE=0.3

# Maximum tokens in AI response (1-8192): Higher = longer responses
CODE_QNA_MAX_OUTPUT_TOKENS=4000

# Enable streaming responses (true/false): Show response as it's generated
CODE_QNA_STREAM=false

# ============================================================================
# Thinking Mode Settings (Gemini 2.5 Series Only)
# ============================================================================

# Enable thinking mode for complex reasoning tasks (true/false)
# Only works with Gemini 2.5 series models (gemini-2.5-flash-preview, gemini-2.5-pro-preview)
# Enables internal reasoning process that improves multi-step analysis
CODE_QNA_ENABLE_THINKING=true

# Thinking token budget for reasoning depth (128-32768 for Pro, 0-24576 for Flash)
# Higher = more detailed reasoning but slower and more expensive
# Gemini 2.5 Pro: minimum 128, cannot be disabled
# Gemini 2.5 Flash: set to 0 to disable thinking
# Recommended: 1024 for balanced performance, 2048+ for complex analysis
CODE_QNA_THINKING_BUDGET=1024

# Include thought summaries in responses (true/false)
# Shows the model's reasoning process - useful for debugging complex questions
# Adds "[THOUGHT]" prefixed content in streaming mode
CODE_QNA_INCLUDE_THOUGHTS=true

# ============================================================================
# Context Optimization Settings
# ============================================================================

# Maximum context size in characters sent to AI (100000-2000000)
# Larger = more context but slower/more expensive. Adjust based on model limits.
CODE_QNA_MAX_CONTEXT=900000

# Buffer percentage for low priority files (0.1-1.0)
# When 90% full, stop adding low-priority files to save space for important ones
CONTEXT_BUFFER_PERCENTAGE=0.9

# Lines of context around each match in partial content (1-50)
# When files are too large, show this many lines around search matches
PARTIAL_CONTENT_WINDOW_SIZE=10

# Maximum total search results to process (50-500)
# Higher = more thorough but slower analysis
MAX_SEARCH_RESULTS=100

# Maximum related files to include via imports (5-50)
# Files discovered through import analysis. Higher = more context but larger size
MAX_RELATED_FILES=10

# Maximum keywords to process from query (3-20)
# Extracts top N keywords from user question for searching
MAX_KEYWORDS_TO_PROCESS=5

# Maximum file size in MB to analyze (0.1-10.0)
# Skip files larger than this to avoid processing huge files
MAX_FILE_SIZE_MB=1.0

# Sample size for cache validation (10-200)
# Check this many files to determine if cache is still valid
CACHE_VALIDATION_SAMPLE_SIZE=50

# Characters per token for estimation (3-6)
# Used to estimate token count. 4 works well for mixed code/text
TOKEN_ESTIMATION_CHARS_PER_TOKEN=4

# ============================================================================
# Search Settings
# ============================================================================

# Maximum results per individual keyword search (10-200)
# Each keyword searches for this many matches before moving to next keyword
SEARCH__MAX_RESULTS_PER_KEYWORD=50

# Lines of context around search matches (5-50)
# Show this many lines above/below each match for better context
SEARCH__MAX_CONTEXT_LINES=15

# Enable fuzzy/approximate matching (true/false)
# Find matches even with small typos or variations
SEARCH__ENABLE_FUZZY_SEARCH=true

# Case sensitive search (true/false)
# Whether "Function" and "function" are treated as different
SEARCH__CASE_SENSITIVE=false

# Search timeout in seconds (10-120)
# Stop searching after this time to avoid hanging
SEARCH__SEARCH_TIMEOUT=30

# Enable semantic code analysis (true/false)
# Slower but finds related code through function calls, imports, etc.
# Requires building semantic graph of codebase
ENABLE_SEMANTIC_ANALYSIS=false

# ============================================================================
# UI Settings
# ============================================================================

# Show debug information (true/false)
# Display detailed progress, timing, and diagnostic info
CODE_QNA_DEBUG=false

# Show file paths in output (true/false)
# Include file paths in responses to help locate code
UI__SHOW_FILE_PATHS=true

# Highlight search matches (true/false)
# Visually emphasize the found keywords in output
UI__HIGHLIGHT_MATCHES=true

# Maximum files to display in results (5-50)
# Limit output size by showing only top N files
UI__MAX_DISPLAY_FILES=10

# Use colors in terminal output (true/false)
# Colorize output for better readability
UI__USE_COLORS=true

# Show line numbers (true/false)
# Include line numbers in code snippets
UI__SHOW_LINE_NUMBERS=true

# Disable all colors (true/false)
# Override color settings - useful for CI/scripts
NO_COLOR=false